# 2023 QA/QC {.unnumbered}

---
execute:
  echo: false
date: "`r Sys.Date()`"
format:
  html:
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
---

```{r, 2023 WQX formatting for SGS, echo = F, message = F}

#| warning: false
#| message: false


# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(readxl)
library(openxlsx)
library(data.table)
library(stringr)
library(magrittr)
library(janitor)
library(hms)
library(lubridate)
library(anytime)
library(stringi)

xfun::pkg_load2(c("htmltools", "mime"))


# Assign 2023 Field Sample Dates 

# Spring 2023 sampling date
spring23_sample_date <- ymd("2023-05-02")

# Summer 2023 Sampling Date
summer23_sample_date <- ymd("2023-07-18")
```

## Introduction

Prior to publishing analysis and interpretation of water quality data, we will ensure that all data that meets QA/QC standards outlined in the current project [Quality Assurance Project Plan (QAPP)](https://www.kenaiwatershed.org/news-media/qapp-revisions-completed-2023/) and is accessible in the appropriate public repository.

Water quality data from this project is ultimately destined for the Environmental Protection Agency's Water Quality Exchange (EPA WQX).

The QAPP for this project describes data management details and responsible parties for each step of the data pipeline from observation to repository. The 2023 data preparation and review process is published here.

### Year 2023 Water Quality Data

In this chapter we will collate 2023 laboratory data from several sources into a single spreadsheet document with a consistent format. The desired end format is a spreadsheet template provided by the EPA Water Quality Exchange. These template files are available to download from the EPA at <https://www.epa.gov/waterdata/water-quality-exchange-web-template-files>.

Once the data is collated, it will be evaluated according to a Quality Assurance Checklist (template example provided by the Alaska Department of Environmental Conservation Soldotna office). Field observations that do not meet the quality assurance standards described in the evaluation checklist will be flagged as "Rejected" before being uploaded to the EPA WQX.

Data that has been uploaded to the EPA WQX is evaluated biannually by the Alaska Department of Environmental Conservation (ADEC) in their [Integrated Water Quality Monitoring and Assessment Report](https://dec.alaska.gov/water/water-quality/integrated-report/)[^2023-1]. The integrated report evaluates available water quality data from the previous five years against Alaska water quality standards and regulations [@adec2020].

[^2023-1]: https://dec.alaska.gov/water/water-quality/integrated-report/

#### 2023 Water Quality Data Formatting

The code scripts in this document assemble water quality data from the three analytical laboratories that partnered with Kenai Watershed Forum for this project in 2023:

-   SGS Laboratories (Anchorage, AK)

-   Soldotna Wastewater Treatment Plant (Soldotna, AK)

<br>

------------------------------------------------------------------------

##### 2023 Metals/Nutrients Lab Results (SGS Labs)

```{r, echo = F, message = F}
library("xfun")

xfun::embed_file('other/input/2023/spring_2023_wqx_data/Data/SGS/Revision 1 SGS Data/1231846_FC5912_Rev1.csv', text = "Download Original Spring 2023 Metals/Nutrients Lab Results from SGS - Electronic Data Delivery file")

xfun::embed_file('other/input/2023/spring_2023_wqx_data/Data/SGS/Revision 1 SGS Data/SGS_Spring_2023_data_final.pdf', text = "Download Original Spring 2023 Metals/Nutrients Lab Results from SGS - PDF file")

```

```{r, echo = F}
xfun::embed_file('other/input/2023/summer_2023_wqx_data/Data/SGS/Summer 2023 SGS Agency Baseline.xlsx', text = "Download Original Summer 2023 Metals/Nutrients Lab Results from SGS - Electronic Data Delivery file")

xfun::embed_file('other/input/2023/summer_2023_wqx_data/Data/SGS/1233640.pdf', text = "Download Original Summer 2023 Metals/Nutrients Lab Results from SGS - PDF file")

xfun::embed_file('other/input/2023/summer_2023_wqx_data/Data/SGS/1233640_COC.pdf', text = "Download Original Summer 2023 Metals/Nutrients Chain of Custody docs from SGS - PDF file")

```

<br>

```{r message = FALSE, echo = F, include = F}

#| warning: false
#| message: false

################################################################################################################
######################################### Read in  and Clean SGS Data ######################################
################################################################################################################


############################ Part A: SGS Data Read In #############################

## Reformat SGS data downloaded from their server client (SGS Engage, full EDD files) to match desired template

# read in
# address column type issues such that both dataframes can be merged
spring_batch_sgs23 <- read.csv("other/input/2023/spring_2023_wqx_data/Data/SGS/Revision 1 SGS Data/1231846_FC5912_Rev1.csv") %>%
  select(-PROJECT_ID,-DISSOLVED)

summer_batch_sgs23 <- read_excel("other/input/2023/summer_2023_wqx_data/Data/SGS/Summer 2023 SGS Agency Baseline.xlsx", sheet = "Sheet9") %>%
  select(-PROJECT_ID,-DISSOLVED) 


# later, need to address different categorical results that were in removed "DISSOLVED" column:

#> unique(spring_batch_sgs23$DISSOLVED)
#[1] TRUE
#> unique(summer_batch_sgs23$DISSOLVED)
#[1] "." "L" "T"

# joining and preparatory steps
## clean up column names and bind seasons together
sgs23 <- bind_rows(spring_batch_sgs23,summer_batch_sgs23) %>%
  clean_names() %>%
  remove_empty() %>%
  
  # add lab name
  mutate(lab_name = "SGS North America, Anchorage, Alaska") %>%
  
  # make 'matrix' column consistent
  select(-matrix) %>%
  mutate(matrix = "Water") %>%
  
  # prepare separate time and date columns for 
  # - sample collection
  # - lab receipt
  # - lab run 
  # - extraction
  
# NOTE summer 2023 lab receipt time/dates missing ?!@
  
  transform(collect_date = mdy_hm(collect_date),
            rec_date = mdy_hm(rec_date),
            run_date = mdy_hm(run_date),
            extracted_date = mdy_hm(extracted_date)) %>%
  separate(collect_date, sep = " ", into = c("collect_date","collect_time")) %>%
  separate(rec_date, sep = " ", into = c("rec_date","rec_time")) %>%
  separate(run_date, sep = " ", into = c("run_date","run_time")) %>%
  separate(extracted_date, sep = " ", into = c("extracted_date","extracted_time"))

# NOTE: in some past years, total metals analyses (method 200.7) have been subcontracted to ALS Laboratories (Seattle). In 2023, SGS Anchorage had the 200.7 methods run by SGS Orlando. Thus, these results are already read in here.


# remove individual dataframe
rm(spring_batch_sgs23,summer_batch_sgs23)
            

################### Part B: Create Consistent Sample Location Names #####################

# export list of unique sample_id
sgs_sitenames <- sgs23 %>%
  distinct(sample_id,collect_date)
colnames(sgs_sitenames) <- c("sample_id","collect_date")
sgs_sitenames %>% arrange(sample_id)
write.csv(sgs_sitenames,"other/input/2023/misc/site_names.csv", row.names = F)

# manually edit site names csv (external to script)


# read in manually edited csv
sgs_sitenames <- read_csv("other/input/2023/misc/site_names_manual_edit.csv") %>%
  filter(!is.na(site_id)) %>%
  # remove trailing spaces
  mutate(sample_id = str_trim(sample_id, side = "right")) %>%
  rename(`site_id  ` = site_id)
colnames(sgs_sitenames) <- c("sample_id","site_id")


#write.csv(sgs_sitenames, "other/input/2023/misc/site_names_manual_edit.csv", row.names = F)
  

# join site_id to main dataframe
sgs23 <- left_join(sgs23,sgs_sitenames, by = "sample_id")

# NOTE: the join for the "No Name Creek" sites are not cooperating.
# After investigating possible reasons, (white spaces, etc) no diagnoses are evident. 
# For time efficiency, we will instead manually rectify this in script with mutate and case_when

sgs23 %<>%
  mutate(site_id = 
           case_when(
             grepl("No Name|NO NAME", sample_id) ~ "KBL_t_00.0",
             TRUE ~ site_id)) 


# join "monitoring location id" to overall sgs23 dataframe
# import example data w/ monitoring location IDs
location_ids <- read.csv("other/output/example_output/results_activities_2021.csv") %>%
  clean_names() %>%
  select(monitoring_location_id, activity_id) %>%
  separate(activity_id, sep = "-", into = "site_id") %>%
  distinct()

# join
sgs23 <- left_join(sgs23,location_ids, by = "site_id") 

# NOTE: the join for several sites are not cooperating.
# These sites include "Skilak Lake Outflow," and "Jim's Landing
# After investigating possible reasons, (white spaces, etc) no diagnoses are evident. 
# For time efficiency, we will instead manually rectify this in script with mutate and case_when

sgs23 %<>%
  transform(monitoring_location_id = as.character(monitoring_location_id)) %>%
  
  # Jim's Landing
  mutate(monitoring_location_id = 
           case_when(
             grepl("JIM'S|Jim's",sample_id) ~ "10000031",
             TRUE ~ monitoring_location_id)) %>%
  
  # Skilak Outlet
  mutate(monitoring_location_id = 
           case_when(
             grepl("Skilak|SKILAK",sample_id) ~ "10000030",
             TRUE ~ monitoring_location_id))

# all 2023 SGS data joined and present.

```

```{r echo = F}

# Activity Type	

# method blank, trip blank, field duplicate)
# this column is addressed out of left to right sequence because its content is needed to generate Activity ID

sgs23 %<>%
  # designate special activity types
  mutate(activity_type = case_when(
    grepl("DUP","Dup", sample_id) ~ "Quality Control Field Replicate Msr/Obs",
    grepl("FB", sample_id) ~ "Quality Control Sample-Field Blank",
    grepl("TRIP BLANK", sample_id) ~ "Quality Control Sample-Trip Blank",
    TRUE ~ "")) %>%
  # designate all other project samples as projet samples (using CDX domain value choices)
  mutate(activity_type = case_when(
    sample_type == "PS" & activity_type == "" ~ "Field Msr/Obs",
    TRUE ~ activity_type))

```


```{r message = FALSE, echo = F, include = F}

# For the remainder of steps, we will conform our data to the columns present in the spreadsheet successfully uploaded with 2021 data proceeding left to right. Columns content is created in one of the following manners:

# Static - column is assigned a static value
# Concatenated - column combines static info from multiple columns
# Calculated - column is dependent on measurements

# NOTE: consider moving some of these steps (ones that apply to all data) to a stage after SWWTP data has been joined.
# Marking candidates for this description with "***"
# If we go this route; leave hashtag column names present as is for sequential navigation, just move code so as reduce redundancy

# Project ID (static)***
sgs23 %<>% mutate(project_id = "10000007")

# Monitoring Location ID (static, already present)

# Activity Media Name	(static)***
sgs23 %<>% mutate(activity_media_name = "Water")

# Activity Media Subdivision Name	(static)***
sgs23 %<>% mutate(activity_media_subdivision_name = "Surface Water")

# Activity ID	(concatenated) (unique identifier) (limit of X  characters)

# due to CDX character limitations, we need to use abbreviations for characteristic (analyte) and sample type
# We will need to distinguish Activity Type (if not normal field obs) and if dissolved

# no column yet designates if status is dissolved (filtered)
# and, notice different methods used in spring vs summer
# may have to relocate this to an earlier step

analytes <- data.frame(unique(sgs23$analyte))



# Activity Start Date	(static)
sgs23 %<>% rename(activity_start_date = collect_date)

# Activity Start Time	(static)
sgs23 %<>% rename(activity_start_time = collect_time)

# Activity Start Time Zone***
sgs23 %<>% mutate (activity_start_time_zone = "AKST")
```


```{r echo = F}
# Perform this step once all above steps are complete and set (un-hashtag)

# Activity Latitude	***
# Activity Longitude	

# (join monitoring_location_id to external spreadsheet table)
#lat_long <- read_csv("other/input/2023/misc/lat_long.csv") %>%
#  transform(monitoring_location_id = as.character(monitoring_location_id))
# z <- sgs23 %>% left_join(sgs23,lat_long, by = "monitoring_location_id")

# 2/1/2024: This step is producing a fatal error that I am unable to diagnose. A typical left-join step instead produces a many-to-many dataframe with hundreds of thousands of rows.
# As a temporary work-around, we will export a csv of the dataframe at this step, manually perform the join, and re-import the csv

# NOTE: we will need to re-perform this manual step each time above chunks change; or if EDD sources change (sheesh)



# export
#sgs23 %<>% arrange(monitoring_location_id)
# write.csv(sgs23, "other/input/2023/misc/temp/sgs23_temp.csv",row.names = F)
# write.csv(lat_long, "other/input/2023/misc/temp/lat_long_temp.csv",row.names = F)

# import manually edited csv and replace sgs23
#sgs23 <- read.csv("other/input/2023/misc/temp/sgs23_temp.csv") %>%
#  select(-X)
```


```{r echo = F}


# Activity Depth/Height Measure	***
sgs23 %<>% mutate(activity_depth_height_measure = "15")

# Activity Depth/Height Unit	***
sgs23 %<>% mutate(activity_depth_height_unit = "cm")

# Activity Comment	
sgs23 %<>% mutate(activity_comment = "")

# Activity Horizontal Collection Method	***
sgs23 %<>% mutate(activity_horizontal_collection_method = "GPS-Unspecified")

# Activity Horizontal Reference Datum	***
sgs23 %<>% mutate(activity_horizontal_reference_datum = "NAD83")

# Characteristic Name	
sgs23 %>% rename(characteristic_name = analyte)

# Result Analytical Method ID	
# (Use table from CDX to match domain values to those names provided by SGS)




# Result Analytical Method Context	
# (Use table from CDX to match domain values to those names provided by SGS)

# Result Value	

# Result Unit	
# rename

# Result Qualifier
# (ensure choices match cdx domain options)

# Result Weight Basis	***
sgs23 %<>% mutate(result_weight_bases = "Sampled")

# Result Sample Fraction	
# (custom match table)

# Method Speciation	
# (custom match table)

# Result Value Type	***
sgs23 %<>% mutate(result_value_type = "Actual")

# Sample Collection Equipment Name	
sgs23 %<>% mutate(sample_collection_equipment_name = "Water Bottle")

# Result Detection Condition	
# categorical choice based on detection limits

# Result Status ID	
# decision made after QA/QC process

# Result Detection Limit Type 1	
# Lower Quantitation Limit

# Result Detection Limit Value 1	

# Result Detection Limit Unit 1	

# Result Detection Limit Type 2	
# Method Detection Level

# Result Detection Limit Value 2	

# Result Detection Limit Unit 2	

# Laboratory Name	***
sgs23 %<>% mutate(laboratory_name  = "SGS North America, Anchorage, Alaska")

# Laboratory Comment Text	
# TBD

# Analysis Start Date	
sgs23 %<>% rename(analysis_start_date = run_date)

# Analysis Start Time	
sgs23 %<>% rename(analysis_start_time = run_time)

# Analysis Start Time Zone	***
sgs23 %<>% mutate(analysis_start_time_zone = "AKST")

# Thermal Preservative Used	***
sgs23 %<>% mutate(thermal_preservative_used = "Cold Packs (4 deg C)")

# Sample Container Type	***
sgs23 %<>% mutate(sample_container_type = "Plastic Bottle")

# Sample Container Color	***
sgs23 %<>% mutate(sample_container_color = "Opaque")

# Chemical Preservative Used ***
sgs23 %<>% mutate(chemical_preservative_used = "HNO3")



```


# check all these again, make decision if static, concatenated, or formulaic 



# general strategy: get all columns of sgs data to conform to final CDX format before joining. (e.g. do all sgs first, then swwtp)

# there are pieces and parts existing in the 2021 script, but to stay organized it would make sense to proceed L --> R for each column


------------------------------------------------------------------------

##### 2023 Fecal Coliform Lab Results (Soldotna Wastewater Treatment Plant Lab)
```{r message = FALSE, echo = F, include = F}
# spring
spring_23_fc_dir <- "other/input/2023/spring_2023_wqx_data/Data/SWWTP/KRWF Fecal 05-02-23.xlsx"
xfun::embed_file(spring_23_fc_dir, text = "Download Original Spring 2023 Fecal Coliform Lab Results from SWWTP")

# summer
summer_23_fc_dir <- "other/input/2023/summer_2023_wqx_data/Data/SWWTP/KRWF Fecal 07-18-23.xlsx"
xfun::embed_file(summer_23_fc_dir, text = "Download Original Summer 2023 Fecal Coliform Lab Results from SWWTP")

```
Chain of custody documents available upon request.

```{r message = FALSE, echo = F, include = F}

############################################################################################################
##################################### Read in  and Clean SWWTP Fecal Coliform Data #########################
############################################################################################################
# spring
spring_23_fc <- read_excel(spring_23_fc_dir, skip = 11, col_types = "text") %>%
  mutate(rec_date = spring23_sample_date) 

# summer
summer_23_fc <- read_excel(summer_23_fc_dir, skip = 11, col_types = "text") %>%
  mutate(rec_date = summer23_sample_date)

# join
fc23 <- bind_rows(spring_23_fc,summer_23_fc) %>%
  clean_names() %>%
  mutate(time_sampled = format(as.POSIXct(Sys.Date() + as.numeric(time_sampled)), "%H:%M", tz="UTC"),
         time_in = format(as.POSIXct(Sys.Date() + as.numeric(time_in)), "%H:%M", tz="UTC"),
         time_out = format(as.POSIXct(Sys.Date() + as.numeric(time_out)), "%H:%M", tz="UTC"))

```

##### 2023 Total Suspended Solids Lab Results (Soldotna Wastewater Treatment Plant (SWWTP))

```{r, echo = F}
# read in

# spring
spring_23_tss_dir <- "other/input/2023/spring_2023_wqx_data/Data/SWWTP/KRWF TSS MONITORING 05-02-23.xlsx"
xfun::embed_file(spring_23_tss_dir, text = "Download Original Spring 2023 Total Suspended Solids Results from SWWTP")

# summer
summer_23_tss_dir <- "other/input/2023/summer_2023_wqx_data/Data/SWWTP/KRWF TSS MONITORING 07-18-23.xlsx"
xfun::embed_file(spring_23_tss_dir, text = "Download Original Summer 2023 Total Suspended Solids Results from SWWTP")
```

Chain of custody documents available upon request

```{r, echo = F}
# read in 2023 TSS data

# spring
spring_23_tss <- read_excel(spring_23_tss_dir, sheet = "Updated_Formatting", skip = 1, col_types = "text") %>%
  mutate(rec_time = as_hms("12:44:00"),
         rec_date = spring23_sample_date)

# summer
summer_23_tss <- read_excel(summer_23_tss_dir, sheet = "Updated_Formatting", skip = 1, col_types = "text") %>%
  mutate(rec_time = as_hms("12:25:00"),
         rec_date = summer23_sample_date,)

# join
tss23 <- bind_rows(spring_23_tss,summer_23_tss) %>%
  
  # clean up names and formats
  clean_names() %>%
  transform(field_sample_date = excel_numeric_to_date(as.numeric(field_sample_date)),
            date_of_analysis = excel_numeric_to_date((as.numeric(date_of_analysis)))) %>%
  mutate(time_sampled = format(as.POSIXct(Sys.Date() + as.numeric(sample_time)), "%H:%M", tz="UTC"),
         time_analysis = format(as.POSIXct(Sys.Date() + as.numeric(time)), "%H:%M", tz="UTC")) %>%
  select(-sample_time,-time,-qc1,-data_entry,-qc2)
  

```






# QA/QC

# Are sample collection dates unique to the actual recorded dates?
# (no; spring has two dates; lab typo)



