# 2023 QA/QC {.unnumbered}

---
execute:
  echo: false
date: "`r Sys.Date()`"
format:
  html:
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
---

```{r, 2023 WQX formatting for SGS, echo = F, message = F}

#| warning: false
#| message: false


# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(readxl)
library(openxlsx)
library(data.table)
library(stringr)
library(magrittr)
library(janitor)
library(hms)
library(lubridate)
library(anytime)
library(stringi)

xfun::pkg_load2(c("htmltools", "mime"))


# Assign 2023 Field Sample Dates 

# Spring 2023 sampling date
spring23_sample_date <- ymd("2023-05-02")

# Summer 2023 Sampling Date
summer23_sample_date <- ymd("2023-07-18")
```

## Introduction

Prior to publishing analysis and interpretation of water quality data, we will ensure that all data that meets QA/QC standards outlined in the current project [Quality Assurance Project Plan (QAPP)](https://www.kenaiwatershed.org/news-media/qapp-revisions-completed-2023/) and is accessible in the appropriate public repository.

Water quality data from this project is ultimately destined for public archival in the Environmental Protection Agency's Water Quality Exchange (EPA WQX).

The QAPP for this project describes data management details and responsible parties for each step of the data pipeline from observation to repository. The 2023 data preparation and review process is published here.

### Year 2023 Water Quality Data

In this chapter we will collate 2023 laboratory data from several sources into a single spreadsheet document with a consistent format. The desired end format is a spreadsheet template provided by the EPA Water Quality Exchange. These template files are available to download from the EPA at <https://www.epa.gov/waterdata/water-quality-exchange-web-template-files>.

Once the data is collated, it will be evaluated according to a Quality Assurance Checklist (template example provided by the Alaska Department of Environmental Conservation Soldotna office). Field observations that do not meet the quality assurance standards described in the evaluation checklist will be flagged as "Rejected" before being uploaded to the EPA WQX.

Data that has been uploaded to the EPA WQX is evaluated biannually by the Alaska Department of Environmental Conservation (ADEC) in their [Integrated Water Quality Monitoring and Assessment Report](https://dec.alaska.gov/water/water-quality/integrated-report/)[^2023-1]. The integrated report evaluates available water quality data from the previous five years against Alaska water quality standards and regulations [@adec2020].

[^2023-1]: https://dec.alaska.gov/water/water-quality/integrated-report/

#### 2023 Water Quality Data Formatting

The code scripts in this document assemble water quality data from the two analytical laboratories that partnered with Kenai Watershed Forum for this project in 2023:

-   SGS Laboratories (Anchorage, AK)

-   Soldotna Wastewater Treatment Plant (Soldotna, AK)

<br>

------------------------------------------------------------------------

##### 2023 Metals/Nutrients Lab Results (SGS Labs)

```{r, echo = F, message = F}
library("xfun")

xfun::embed_file('other/input/2023/spring_2023_wqx_data/Data/SGS/Revision 1 SGS Data/1231846_FC5912_Rev1.csv', text = "Download Original Spring 2023 Metals/Nutrients Lab Results from SGS - Electronic Data Delivery file")

xfun::embed_file('other/input/2023/spring_2023_wqx_data/Data/SGS/Revision 1 SGS Data/SGS_Spring_2023_data_final.pdf', text = "Download Original Spring 2023 Metals/Nutrients Lab Results from SGS - PDF file")

```

```{r, echo = F}
xfun::embed_file('other/input/2023/summer_2023_wqx_data/Data/SGS/Summer 2023 SGS Agency Baseline.xlsx', text = "Download Original Summer 2023 Metals/Nutrients Lab Results from SGS - Electronic Data Delivery file")

xfun::embed_file('other/input/2023/summer_2023_wqx_data/Data/SGS/1233640.pdf', text = "Download Original Summer 2023 Metals/Nutrients Lab Results from SGS - PDF file")

xfun::embed_file('other/input/2023/summer_2023_wqx_data/Data/SGS/1233640_COC.pdf', text = "Download Original Summer 2023 Metals/Nutrients Chain of Custody docs from SGS - PDF file")

```

<br>

```{r message = FALSE, echo = F, include = F}

#| warning: false
#| message: false


############################ Part A.1: SGS 2023 Data Read In #############################

## Reformat SGS data downloaded from their server client (SGS Engage, full EDD files) to match desired template

# read in
# address column type issues such that both dataframes can be merged
spring_batch_sgs23 <- read.csv("other/input/2023/spring_2023_wqx_data/Data/SGS/Revision 1 SGS Data/1231846_FC5912_Rev1.csv") %>%
  select(-PROJECT_ID,-DISSOLVED) %>%
  rename(RUN_DATE_TIME = RUN_DATE) %>% # check to see if this column name is consistent
  
# NOTE the receive date/time is incorrect in the EDD. The EDD shows 5/9/23 for all project samples (sample_type = "PS") and time is not specified. The PDF report indicates all samples were in fact received on 5/04/23 08:51, which is consistent with the chain of custody docs.
# We will apply this correction here
  mutate(REC_DATE = case_when(
    SAMPLE_TYPE == "PS" ~ "5/04/23 08:51",
    TRUE ~ REC_DATE))
  
summer_batch_sgs23 <- read_excel("other/input/2023/summer_2023_wqx_data/Data/SGS/Summer 2023 SGS Agency Baseline.xlsx", sheet = "Sheet9") %>%
  select(-PROJECT_ID,-DISSOLVED) 


# later, need to address different categorical results that were in removed "DISSOLVED" column:

#> unique(spring_batch_sgs23$DISSOLVED)
#[1] TRUE
#> unique(summer_batch_sgs23$DISSOLVED)
#[1] "." "L" "T"

# joining and preparatory steps
## clean up column names and bind seasons together
sgs23 <- bind_rows(spring_batch_sgs23,summer_batch_sgs23) %>%
  clean_names() %>%
  remove_empty() %>%
  
  # add lab name
  mutate(lab_name = "SGS North America, Anchorage, Alaska") %>%
  
  # make 'matrix' column consistent
  select(-matrix) %>%
  mutate(matrix = "Water") %>%
  
  # prepare separate time and date columns for 
  # - sample collection
  # - lab receipt
  # - lab run 
  # - extraction
  

  
  transform(collect_date = mdy_hm(collect_date),
            rec_date = mdy_hm(rec_date),
            run_date = mdy_hm(run_date_time),
            extracted_date = mdy_hm(extracted_date)) %>%
  separate(collect_date, sep = " ", into = c("collect_date","collect_time")) %>%
  separate(rec_date, sep = " ", into = c("rec_date","rec_time")) %>%
  separate(run_date_time, sep = " ", into = c("run_date_time","run_time")) %>%
  separate(extracted_date, sep = " ", into = c("extracted_date","extracted_time"))

# NOTE: in some past years, total metals analyses (method 200.7) have been subcontracted to ALS Laboratories (Seattle). In 2023, SGS Anchorage had the 200.7 methods run by SGS Orlando. Thus, these results are already read in here.


# remove individual dataframe
rm(spring_batch_sgs23,summer_batch_sgs23)
            

################### Part A.2: Create Consistent Sample Location Names for SGS data #####################

# export list of unique sample_id
sgs_sitenames <- sgs23 %>%
  distinct(sample_id,collect_date)
colnames(sgs_sitenames) <- c("sample_id","collect_date")
sgs_sitenames %>% arrange(sample_id)
write.csv(sgs_sitenames,"other/input/2023/misc/site_names.csv", row.names = F)

# manually edit site names csv (external to script)


# read in manually edited csv
sgs_sitenames <- read_csv("other/input/2023/misc/site_names_manual_edit_sgs23.csv") %>%
  filter(!is.na(site_id)) %>%
  # remove trailing spaces
  mutate(sample_id = str_trim(sample_id, side = "right")) %>%
  rename(`site_id  ` = site_id)
colnames(sgs_sitenames) <- c("sample_id","site_id")


#write.csv(sgs_sitenames, "other/input/2023/misc/site_names_manual_edit.csv", row.names = F)
  

# join site_id to main dataframe
sgs23 <- left_join(sgs23,sgs_sitenames, by = "sample_id")

# NOTE: the join for the "No Name Creek" sites are not cooperating.
# After investigating possible reasons, (white spaces, etc) no diagnoses are evident. 
# For time efficiency, we will instead manually rectify this in script with mutate and case_when

sgs23 %<>%
  mutate(site_id = 
           case_when(
             grepl("No Name|NO NAME", sample_id) ~ "KBL_t_00.0",
             TRUE ~ site_id)) 


# join "monitoring location id" to overall sgs23 dataframe
# import example data w/ monitoring location IDs
location_ids <- read.csv("other/output/example_output/results_activities_2021.csv") %>%
  clean_names() %>%
  select(monitoring_location_id, activity_id) %>%
  separate(activity_id, sep = "-", into = "site_id") %>%
  distinct()

# join
sgs23 <- left_join(sgs23,location_ids, by = "site_id") 

# NOTE: the join for several sites are not cooperating.
# These sites include "Skilak Lake Outflow," and "Jim's Landing
# After investigating possible reasons, (white spaces, etc) no diagnoses are evident. 
# For time efficiency, we will instead manually rectify this in script with mutate and case_when

sgs23 %<>%
  transform(monitoring_location_id = as.character(monitoring_location_id)) %>%
  
  # Jim's Landing
  mutate(monitoring_location_id = 
           case_when(
             grepl("JIM'S|Jim's",sample_id) ~ "10000031",
             TRUE ~ monitoring_location_id)) %>%
  
  # Skilak Outlet
  mutate(monitoring_location_id = 
           case_when(
             grepl("Skilak|SKILAK",sample_id) ~ "10000030",
             TRUE ~ monitoring_location_id))

# all 2023 SGS data joined and present in same dataframe


############### Part A.3: Assign miscellaneous calculated columns for 2023 SGS data (needed for next steps) ###############


# Activity Type	

# method blank, trip blank, field duplicate)
# this column is addressed out of left to right sequence because its content is needed to generate Activity ID

sgs23 %<>%
  # designate special activity types
  mutate(activity_type = case_when(
    grepl("DUP|Dup", sample_id) ~ "Quality Control Field Replicate Msr/Obs",
    grepl("FB", sample_id) ~ "Quality Control Sample-Field Blank",
    grepl("TRIP BLANK", sample_id) ~ "Quality Control Sample-Trip Blank",
    TRUE ~ "")) %>%
  # designate all other project samples as project samples (using CDX domain value choices)
  mutate(activity_type = case_when(
    sample_type == "PS" & activity_type == "" ~ "Field Msr/Obs",
    TRUE ~ activity_type)) %>%
  # designate activity type abbreviations for late use in actvity ID
  mutate(activity_type_abbrv = case_when(
    activity_type == "Quality Control Field Replicate Msr/Obs" ~ "DUP",
    activity_type == "Quality Control Sample-Field Blank" ~ "FB",
    activity_type == "Quality Control Sample-Trip Blank" ~ "TB",
    TRUE ~ ""
  ))
  

# Result Sample Fraction	
# CDX domains: "Filtered, field," "Filtered, lab,"  "Unfiltered," "Volatile", "Total"
sgs23 %<>%
  mutate(result_sample_fraction = case_when(
    grepl("EP200.8", analytical_method) ~ "Filtered, lab",              # dissolved metals
    grepl("EPA 200.7|SW846 6010D", analytical_method) ~ "Unfiltered",   # total metals
    grepl("SW8260D", analytical_method) ~ "Volatile",                   # hydrocarbons
    grepl("SM21 4500NO3-F|SM21 4500P-B,E", analytical_method) ~ "Total",# total N and P
    TRUE ~ "")) %>%
  
  # designate dissolved vs. total metals (filtered vs. unfiltered) for use in concatenating Activity ID
  mutate(diss_abbrv = case_when(
    result_sample_fraction == "Filtered, lab" ~ "Diss",
    TRUE ~ ""))

```


```{r message = FALSE, echo = F, include = F}

################### Part A.4: Proceed left to right across column names to calculate or assign as needed ###################

# For the remainder of steps, we will conform our data to the columns present in the spreadsheet successfully uploaded with 2021 data proceeding left to right. Columns content is created in one of the following manners:

# Static - column is assigned a static value
# Concatenated - column combines static info from multiple columns
# Calculated - column is dependent on measurements

# NOTE: column names marked with triple asterisk "***" are common to all dataframes, and thus are done in bulk at the end of steps that are not common to all dataframes


# Project ID (static)***

# Monitoring Location ID (static, already present)

# Activity Media Name	(static)***

# Activity Media Subdivision Name	(static)***

# Activity ID	(concatenated) (unique identifier) (limit of X  characters)
# due to CDX character limitations, we need to use abbreviations for 1) characteristic (analyte) and 2) sample type
# import analyte abbreviations and join
# note that surrogate organic compunds ("surr" suffix) do not have analyte abbreviations; becuase they are not project analytes
analyte_abbrv <- read.csv("other/input/2023/misc/analytes_list_manual_edit.csv") %>%
  filter(analyte_abbreviation != "")
colnames(analyte_abbrv) <- c("analyte","analyte_abbrv")
sgs23 %<>% left_join(analyte_abbrv)
  
# create Activity ID column
sgs23 %<>%
  mutate(activity_id = case_when(
    activity_type_abbrv == "" ~ paste0(site_id,"-",collect_date,"-",analyte_abbrv),
    TRUE ~ paste0(site_id,"-",collect_date,"-",analyte_abbrv,"-",activity_type_abbrv))) %>%
  mutate(activity_id = case_when(
    diss_abbrv == "" ~ activity_id,
    TRUE ~ paste0(activity_id,"-",diss_abbrv)))

# Activity Start Date	(static, specific to SGS)
sgs23 %<>% rename(activity_start_date = collect_date)

# Activity Start Time	(static, specific to SGS)
sgs23 %<>% rename(activity_start_time = collect_time)

# Activity Start Time Zone***

# Activity Depth/Height Measure	***

# Activity Depth/Height Unit	***

# Activity Comment ***

# Activity Horizontal Collection Method	***

# Activity Horizontal Reference Datum	***

# Characteristic Name	
sgs23 %<>% rename(characteristic_name = analyte)

# Result Analytical Method ID,	and
# Result Analytical Method Context	
# (Use table from CDX to match domain values to those names provided by SGS)

# read in table
analytical_methods_contexts <- read.csv("other/input/2023/misc/analytical_method_id_and_context_matching_table.csv") %>%
  select(-comments,-EPA.name) %>%
  filter(analytical_method != "") %>%
  mutate(analytical_method = str_trim(analytical_method)) # remove white spaces !

# join to sgs23
sgs23 %<>% left_join(analytical_methods_contexts)

# Result Value	
sgs23 %<>% rename(result_value = result)

# Result Unit	
sgs23 %<>% rename(result_unit = units)

# Result Qualifier
# options are "U"  "N"  "*N" "*"  "="  "J". Modify to ensure choices match CDX domain options.
# Note: *N" is not an option present in CDX; all other choices are. We will modify "*N" to simply "*" for simplification, for it's one instance as an MSD sample (matrix spike duplicate; non-field). Rationale: the "N" part of this qualifier is not mandatory information.
sgs23 %<>% 
  mutate(result_qualifier = case_when(
    resultflag == "*N" ~ "*",
    TRUE ~ resultflag))


# Result Weight Basis	***

# Result Sample Fraction	
## already completed in a previous step

# Method Speciation	
## only some parameters in CDX require that speciation is made explicit. Following example from 2021 experience
sgs23 %<>%
  mutate(method_speciation = case_when(
    result_analytical_method_id == "4500-NO3(F)" ~ "as NO2+NO3",
    result_analytical_method_id == "4500-P-E" ~ "as P",
    TRUE ~ ""))

# Result Value Type	***

# Sample Collection Equipment Name ***	

# Result Detection Condition	
# categorical choice based on result qualifier. 
# Attempt to use matching table generates an erroneous many-to-many join for unknown reasons, so we will instead use "case_when"
# result_detection_condition <- read.csv("other/input/2023/misc/result_detection_condition.csv")
sgs23 %<>% 
  mutate(result_detection_condition = case_when(
    result_qualifier == "U" ~ "Not Detected",
    result_qualifier == "J" ~ "Present Below Quantification Limit",
    result_qualifier == "N" ~ "Presumptive evidence of a nontarget compound",
    result_qualifier == "*" ~ "Sample was warm when received"))


# Result Status ID	(make as very last step?)
# decision made after QA/QC process
# TBD work to be completed here <--------------------------------------------

# Note: the acronyms & terminology for detection limits provided by SGS differ slightly from those required by CDX

# Result Detection Limit Type 1	
# Lower Quantitation Limit (listed as "loq", or "Limit of Quantitation")
sgs23 %<>% mutate(result_detection_limit_type_1 = "Lower Quantitation Limit")

# Result Detection Limit Value 1	
sgs23 %<>% rename(result_detection_limit_value_1 = loq)

# Result Detection Limit Unit 1	
sgs23 %<>% mutate(result_detection_limit_unit_1 = result_unit)

# Result Detection Limit Type 2	
# Method Detection Level (listed as "lod", or "limit of detection")
sgs23 %<>% mutate(result_detection_limit_type_2 = "Method Detection Level")

# Result Detection Limit Value 2	
sgs23 %<>% rename(result_detection_limit_value_2 = lod)

# Result Detection Limit Unit 2	
sgs23 %<>% mutate(result_detection_limit_unit_2 = result_unit)

# Laboratory Name	
sgs23 %<>% mutate(laboratory_name  = "SGS North America, Anchorage, Alaska")

# Laboratory Comment Text	
# TBD
sgs23 %<>% mutate(lab_comment_text = "")

# Analysis Start Date	
sgs23 %<>% rename(analysis_start_date = run_date_time)

# Analysis Start Time	
sgs23 %<>% rename(analysis_start_time = run_time)

# Analysis Start Time Zone (can vary by year if subcontract lab used)
sgs23 %<>% mutate(analysis_start_time_zone = "AKST")

# Thermal Preservative Used	***

# Sample Container Type	
sgs23 %<>% 
  mutate(sample_container_type = case_when(
  result_analytical_method_id == "8260D" ~ "Glass Vial",
  TRUE ~ "Plastic Bottle"))

# Sample Container Color
sgs23 %<>% 
  mutate(sample_container_color = case_when(
  result_analytical_method_id == "8260D" ~ "Amber",
  TRUE ~ "Opaque"))
  

# Chemical Preservative Used 
# note that 200.8 samples were lab-filtered in spring & summer 2023, so no field preservative was used (would normally be HNO3 for year prior to 2022)
sgs23 %<>% 
  mutate(chemical_preservative_used = case_when(
      result_analytical_method_id == "8260D" ~ "HCl",
      result_analytical_method_id == "4500-NO3(F)" ~ "H2SO4",
      result_analytical_method_id == "4500-P-E" ~ "H2SO4",
      TRUE ~ ""))

```



--> use EPA metals guidance in QA/QC rather than at this stage!



# check all these again, make decision if static, concatenated, or formulaic 

# general strategy: get all columns of sgs data to conform to final CDX format before joining. (e.g. do all sgs first, then swwtp)

# there are pieces and parts existing in the 2021 script, but to stay organized it would make sense to proceed L --> R for each column




------------------------------------------------------------------------

##### 2023 Fecal Coliform Lab Results (Soldotna Wastewater Treatment Plant Lab)
```{r message = FALSE, echo = F, include = F}

######################### Part B: SWWTP Fecal Coliform Data ############################

# spring
spring_23_fc_dir <- "other/input/2023/spring_2023_wqx_data/Data/SWWTP/KRWF Fecal 05-02-23.xlsx"
xfun::embed_file(spring_23_fc_dir, text = "Download Original Spring 2023 Fecal Coliform Lab Results from SWWTP")

# summer
summer_23_fc_dir <- "other/input/2023/summer_2023_wqx_data/Data/SWWTP/KRWF Fecal 07-18-23.xlsx"
xfun::embed_file(summer_23_fc_dir, text = "Download Original Summer 2023 Fecal Coliform Lab Results from SWWTP")

```
Chain of custody documents available upon request.

```{r message = FALSE, echo = F, include = F}

############################ Part B.1: SWWTP Fecal Coliform Data Read In #############################

# spring
spring_23_fc <- read_excel(spring_23_fc_dir, skip = 11, col_types = "text") %>%
  mutate(rec_date = spring23_sample_date) 

# summer
summer_23_fc <- read_excel(summer_23_fc_dir, skip = 11, col_types = "text") %>%
  mutate(rec_date = summer23_sample_date)

# join
fc23 <- bind_rows(spring_23_fc,summer_23_fc) %>%
  clean_names() %>%
  # standardize time formats
  mutate(time_sampled = format(as.POSIXct(Sys.Date() + as.numeric(time_sampled)), "%H:%M", tz="UTC"),
         time_in = format(as.POSIXct(Sys.Date() + as.numeric(time_in)), "%H:%M", tz="UTC"),
         time_out = format(as.POSIXct(Sys.Date() + as.numeric(time_out)), "%H:%M", tz="UTC"),
         rec_time = format(as.POSIXct(Sys.Date() + as.numeric(rec_time)), "%H:%M", tz="UTC"))

```


```{r echo = F}

################### Part B.2: Create Consistent Sample Location Names for SWWTP FC data #####################

# execute once and manually modify csv to create match table
# z <- sgs23 %>%
#  select(site_id,monitoring_location_id) %>%
#  distinct()
# write.csv(z,"other/input/2023/misc/site_names_manual_edit_fc23.csv", row.names = F)

# read in fc sites
fc23_site_names <- read.csv("other/input/2023/misc/site_names_manual_edit_fc23.csv") %>%
  remove_empty()

# join to fecal coliform data frame
fc23 %<>% left_join(fc23_site_names)



############### Part B.3: Assign miscellaneous columns for 2023 SWWTP FC (needed for next steps) ###############

# Activity Type	

# method blank, trip blank, field duplicate)
# this column is addressed out of left to right sequence because its content is needed to generate Activity ID

fc23 %<>%
  mutate(activity_type = case_when(
    grepl("DUP", sample_location_rm) ~ "Quality Control Field Replicate Msr/Obs",
    grepl("BLANK", sample_location_rm) ~ "Quality Control Sample-Lab Blank",
    grepl("POSITIVE", sample_location_rm) ~ "Sample-Positive Control",
    TRUE ~ "Field Msr/Obs")) %>%
# designate activity type abbreviations for late use in actvity ID
  mutate(activity_type_abbrv = case_when(
    activity_type == "Quality Control Field Replicate Msr/Obs" ~ "DUP",
    activity_type == "Quality Control Sample-Lab Blank" ~ "LB",
    activity_type == "Sample-Positive Control" ~ "POS",
    TRUE ~ ""))


# Result Sample Fraction	
# CDX domains: none used in 2021 example
fc23 %<>% mutate(result_sample_fraction = "")

# Collect Date
fc23 %<>% rename(collect_date = rec_date)



################### Part B.4: Proceed left to right across column names to calculate as needed #####################


# NOTE: column names marked with triple asterisk "***" are common to all dataframes, and thus are done in bulk at the end of steps that are not common to all dataframes


# Project ID (static)***

# Monitoring Location ID (static, already present)

# Activity Media Name	(static)***

# Activity Media Subdivision Name	(static)***

# Activity ID	(concatenated) (unique identifier) (limit of X  characters)
# due to CDX character limitations, we need to use abbreviations for 1) characteristic (analyte) and 2) sample type
fc23 %<>% mutate(analyte_abbrv = "FC")
  
# create Activity ID column
fc23 %<>%
  mutate(activity_id = case_when(    
    grepl("POS|LB", activity_type_abbrv) ~ "",
    activity_type_abbrv == "" ~ paste0(site_id,"-",collect_date,"-",analyte_abbrv),
    TRUE ~ paste0(site_id,"-",collect_date,"-",analyte_abbrv,"-",activity_type_abbrv))) 
  

# Activity Start Date	
fc23 %<>% rename(activity_start_date = collect_date)

# Activity Start Time	(static, specific to SGS)
fc23 %<>% rename(activity_start_time = time_sampled)

# Activity Start Time Zone***

# Activity Depth/Height Measure	***

# Activity Depth/Height Unit	***

# Activity Comment ***

# Activity Horizontal Collection Method	***

# Activity Horizontal Reference Datum	***

# Characteristic Name	
fc23 %<>% mutate(characteristic_name = "Fecal Coliform")

# Result Analytical Method ID,	and
# Result Analytical Method Context	
fc23 %<>%
  mutate(result_analytical_method_id = "9222D",
         result_analytical_method_context = "APHA")


# Result Value	
fc23 %<>% rename(result_value = colony_count_100m_l)

 # Result Unit	
fc23 %<>% mutate(result_unit = "cfu/100ml")

# Result Qualifier
# Use "=" if result > 1 and "U" if result < 1. See pg 17 of 2020 QAPP at "other/documents/QAPP/QAPP-2020-KenaiRiverWatershed_ZnCu.pdf" in Appendix A of comprehensive report
fc23 %<>%
  mutate(result_qualifier = case_when(
    result_value < 1 ~ "U",
    TRUE ~ "="))
 

# Result Weight Basis	***

# Result Sample Fraction	
## already completed in a previous step

# Method Speciation	
# not applicable to fecal coliform

# Result Value Type	***

# Sample Collection Equipment Name ***	

# Result Detection Condition	
# categorical choice based on result qualifier. 
# Attempt to use matching table generates an erroneous many-to-many join for unknown reasons, so we will instead use "case_when"
# result_detection_condition <- read.csv("other/input/2023/misc/result_detection_condition.csv")
fc23 %<>% 
  mutate(result_detection_condition = case_when(
    result_qualifier == "U" ~ "Not Detected",
    result_qualifier == "J" ~ "Present Below Quantification Limit",
    TRUE ~ ""))


# Result Status ID	(make as very last step?)
# decision made after QA/QC process
# TBD work to be completed here <--------------------------------------------

# Note: the acronyms & terminology for detection limits provided by SGS differ slightly from those required by CDX

# Result Detection Limit Type 1	
# Lower Quantitation Limit (listed as "loq", or "Limit of Quantitation")
fc23 %<>% mutate(result_detection_limit_type_1 = "Lower Quantitation Limit")

# Result Detection Limit Value 1	
fc23 %<>% mutate(result_detection_limit_value_1 = 1)

# Result Detection Limit Unit 1	
fc23 %<>% mutate(result_detection_limit_unit_1 = result_unit)

# Result Detection Limit Type 2	
# Method Detection Level (listed as "lod", or "limit of detection")
fc23 %<>% mutate(result_detection_limit_type_2 = "")

# Result Detection Limit Value 2	
fc23 %<>% mutate(result_detection_limit_value_2 = "")

# Result Detection Limit Unit 2	
fc23 %<>% mutate(result_detection_limit_unit_2 = result_unit)

# Laboratory Name	
fc23 %<>% mutate(laboratory_name  = "Soldotna Wastewater Treatment Plant, Soldotna, Alaska")

# Laboratory Comment Text	
# TBD
fc23 %<>% mutate(lab_comment_text = notes)

# Analysis Start Date	
# all FC samples are analyzed the same day as received by the lab
fc23 %<>% mutate(analysis_start_date = activity_start_date)

# Analysis Start Time	
fc23 %<>% rename(analysis_start_time = time_in)

# Analysis Start Time Zone (can vary by year if subcontract lab used)
fc23 %<>% mutate(analysis_start_time_zone = "AKST")

# Thermal Preservative Used	***

# Sample Container Type	
fc23 %<>% mutate(sample_container_type = "Plastic Bottle")

# Sample Container Color	
fc23 %<>% mutate(sample_container_color = "Clear")

# Chemical Preservative Used 
fc23 %<>% mutate(chemical_preservative_used = "")

# date received at lab is same as activity datef
fc23 %<>% mutate(rec_date = activity_start_date)

```


<br>




##### 2023 Total Suspended Solids Lab Results (Soldotna Wastewater Treatment Plant (SWWTP))

```{r, echo = F}

############################ Part C: SWWTP Fecal Coliform Data #############################

# read in

# spring
spring_23_tss_dir <- "other/input/2023/spring_2023_wqx_data/Data/SWWTP/KRWF TSS MONITORING 05-02-23.xlsx"
xfun::embed_file(spring_23_tss_dir, text = "Download Original Spring 2023 Total Suspended Solids Results from SWWTP")

# summer
summer_23_tss_dir <- "other/input/2023/summer_2023_wqx_data/Data/SWWTP/KRWF TSS MONITORING 07-18-23.xlsx"
xfun::embed_file(spring_23_tss_dir, text = "Download Original Summer 2023 Total Suspended Solids Results from SWWTP")
```

Chain of custody documents available upon request

```{r, echo = F}
# read in 2023 TSS data

# spring
spring_23_tss <- read_excel(spring_23_tss_dir, sheet = "Updated_Formatting", skip = 1, col_types = "text") %>%
  # see "ReadMe" document at other\input\2023\spring_2023_wqx_data\Chain of Custody Forms\completed forms for rec_time rationale
  mutate(rec_time = case_when(
    grepl("RM_1.5",Sample_Location) ~ as_hms("15:30:00"),
    TRUE ~ as_hms("12:44:00"))) %>%
  mutate(rec_date = spring23_sample_date)

# summer
summer_23_tss <- read_excel(summer_23_tss_dir, sheet = "Updated_Formatting", skip = 1, col_types = "text") %>%
  mutate(rec_time = as_hms("12:25:00"),
         rec_date = summer23_sample_date,)

# join
tss23 <- bind_rows(spring_23_tss,summer_23_tss) %>%
  
  # clean up names and formats
  clean_names() %>%
  transform(field_sample_date = excel_numeric_to_date(as.numeric(field_sample_date)),
            date_of_analysis = excel_numeric_to_date((as.numeric(date_of_analysis)))) %>%
  mutate(time_sampled = format(as.POSIXct(Sys.Date() + as.numeric(sample_time)), "%H:%M", tz="UTC"),
         time_analysis = format(as.POSIXct(Sys.Date() + as.numeric(time)), "%H:%M", tz="UTC")) %>%
  select(-sample_time,-time,-qc1,-data_entry,-qc2)
  
```



```{r echo = F}

################### Part C.2: Create Consistent Sample Location Names for SWWTP TSS data #####################

# execute once and manually modify csv to create match table
#z <- tss23 %>%
#  select(sample_location) %>%
#  distinct()
#write.csv(z,"other/input/2023/misc/site_names_manual_edit_tss23.csv", row.names = F)

# read in tss sites
tss23_site_names <- read.csv("other/input/2023/misc/site_names_manual_edit_tss23.csv") %>%
  remove_empty()

# join site names to tss data frame
tss23 %<>% left_join(tss23_site_names)


############### Part C.3: Assign miscellaneous calculated columns for 2023 SWWTP TSS (needed for next steps) ###############

# Activity Type	

# method blank, field duplicate)
# this column is addressed out of left to right sequence because its content is needed to generate Activity ID

tss23 %<>%
  mutate(activity_type = case_when(
    grepl("DUP", sample_location) ~ "Quality Control Field Replicate Msr/Obs",
    grepl("BLANK", sample_location) ~ "Quality Control Sample-Lab Blank",
    TRUE ~ "Field Msr/Obs")) %>%
# designate activity type abbreviations for late use in actvity ID
  mutate(activity_type_abbrv = case_when(
    activity_type == "Quality Control Field Replicate Msr/Obs" ~ "DUP",
    activity_type == "Quality Control Sample-Lab Blank" ~ "LB",
    TRUE ~ ""))


# Result Sample Fraction	
# CDX domains: none used in 2021 example
tss23 %<>%
  mutate(result_sample_fraction = "")


# Collect Date
tss23 %<>% rename(collect_date = field_sample_date)


################### Part C.4: Proceed left to right across column names to calculate as needed #####################


# NOTE: column names marked with triple asterisk "***" are common to all dataframes, and thus are done in bulk at the end of steps that are not common to all dataframes


# Project ID (static)***

# Monitoring Location ID (static, already present)

# Activity Media Name	(static)***

# Activity Media Subdivision Name	(static)***

# Activity ID	(concatenated) (unique identifier) (limit of X  characters)
# due to CDX character limitations, we need to use abbreviations for 1) characteristic (analyte) and 2) sample type
tss23 %<>% mutate(analyte_abbrv = "TSS")
  
# create Activity ID column
tss23 %<>%
  mutate(activity_id = case_when(    
    grepl("DUP", activity_type_abbrv) ~ paste0(site_id,"-",collect_date,"-",analyte_abbrv,"-",activity_type_abbrv),
    grepl("LB", activity_type_abbrv) ~ "",
    TRUE ~  paste0(site_id,"-",collect_date,"-",analyte_abbrv))) 
  

# Activity Start Date	
tss23 %<>% rename(activity_start_date = collect_date)

# Activity Start Time	(static, specific to SGS)
tss23 %<>% rename(activity_start_time = time_sampled)

# Activity Start Time Zone***

# Activity Depth/Height Measure	***

# Activity Depth/Height Unit	***

# Activity Comment ***

# Activity Horizontal Collection Method	***

# Activity Horizontal Reference Datum	***

# Characteristic Name	
tss23 %<>% mutate(characteristic_name = "Total suspended solids")

# Result Analytical Method ID,	and
# Result Analytical Method Context	
tss23 %<>%
  mutate(result_analytical_method_id = "2540-D",
         result_analytical_method_context = "APHA")


# Result Value	
tss23 %<>% rename(result_value = s_s_mg_l)


 # Result Unit	
tss23 %<>% mutate(result_unit = "mg/L")

# Result Qualifier
tss23 %<>%
  mutate(result_qualifier = case_when(
    result_value < 1 & result_value > 0.31 ~ "J",
    result_value < 0.31 ~ "U"))

# Result Weight Basis	***

# Result Sample Fraction	
## already completed in a previous step

# Method Speciation	
# not applicable to TSS

# Result Value Type	***

# Sample Collection Equipment Name ***	

# Result Detection Condition	
# categorical choice based on result qualifier. 
# Attempt to use matching table generates an erroneous many-to-many join for unknown reasons, so we will instead use "case_when"
# result_detection_condition <- read.csv("other/input/2023/misc/result_detection_condition.csv")
tss23 %<>% 
  mutate(result_detection_condition = case_when(
    result_qualifier == "U" ~ "Not Detected",
    result_qualifier == "J" ~ "Present Below Quantification Limit",
    TRUE ~ ""))


# Result Status ID	(make as very last step?)
# decision made after QA/QC process
# TBD work to be completed here <--------------------------------------------

# Note: the acronyms & terminology for detection limits provided by SGS differ slightly from those required by CDX

# Result Detection Limit Type 1	
# Lower Quantitation Limit (listed as "loq", or "Limit of Quantitation")
tss23 %<>% mutate(result_detection_limit_type_1 = "Lower Quantitation Limit")

# Result Detection Limit Value 1	
tss23 %<>% mutate(result_detection_limit_value_1 = 1)

# Result Detection Limit Unit 1	
tss23 %<>% mutate(result_detection_limit_unit_1 = result_unit)

# Result Detection Limit Type 2	
# Method Detection Level (listed as "lod", or "limit of detection")
tss23 %<>% mutate(result_detection_limit_type_2 = "Method Detection Level")

# Result Detection Limit Value 2	
tss23 %<>% mutate(result_detection_limit_value_2 = 0.31)

# Result Detection Limit Unit 2	
tss23 %<>% mutate(result_detection_limit_unit_2 = result_unit)

# Laboratory Name	
tss23 %<>% mutate(laboratory_name  = "Soldotna Wastewater Treatment Plant, Soldotna, Alaska")

# Laboratory Comment Text	
# TBD
tss23 %<>% mutate(lab_comment_text = notes)

# Analysis Start Date	
# all FC samples are analyzed the same day as received by the lab
tss23 %<>% rename(analysis_start_date = date_of_analysis)

# Analysis Start Time	
tss23 %<>% rename(analysis_start_time = time_analysis)

# Analysis Start Time Zone (can vary by year if subcontract lab used)
tss23 %<>% mutate(analysis_start_time_zone = "AKST")

# Thermal Preservative Used	***

# Sample Container Type	
tss23 %<>% mutate(sample_container_type = "Plastic Bottle")

# Sample Container Color	
tss23 %<>% mutate(sample_container_color = "Opaque")

# Chemical Preservative Used 
tss23 %<>% mutate(chemical_preservative_used = "")

```

```{r echo =F}
# remove extraneous dataframes
rm(fc23_site_names,sgs_sitenames,spring_23_fc,spring_23_tss,summer_23_fc,summer_23_tss,tss23_site_names)

```



```{r echo = F}

################# Part D: Join Separate Datasets into shared dataframe (SGS, Fecal Coliform, TSS) ###################

# strategy - 1) retain only needed columns (2021 example), then 2) assign consistent column classes (temp loggers example)


# subset dataframes to limited set of desired final colnames based on 2021 example
# read in column names
col_names <- read.csv("other/output/example_output/results_activities_2021.csv") %>%
  clean_names() 
  
# For QA/QC purposes only, we also want to various columns:

# a.) retain the "sample type" column; which contains information about the purpose of various lab analyses (e.g. method blanks, matrix spikes, etc). This column is included in the SGS data deliverable, and is manually added to the other spreadsheets from the wastewater treatment plant
col_names %<>% mutate(sample_type = "")


# b.) retain info about when samples were received to laboratories (for hold time calculations)
col_names %<>% mutate(rec_time = "",
                      rec_date = "")

# convert column names to list
col_names %<>% colnames()


# retain desired columns from each dataframe 
# and, convert all columns to character
sgs23 %<>% 
  select(one_of(col_names)) %>% 
  mutate(across(everything(), as.character))

fc23 %<>% 
  select(one_of(col_names)) %>%
  mutate(across(everything(), as.character))

tss23 %<>% 
  select(one_of(col_names)) %>%
  mutate(across(everything(), as.character))

# combine
dat <- bind_rows(sgs23,fc23,tss23)

```




```{r echo = F}

##################### Part E: Assign Information Common to All 2023 Datasets #########################


# Static additions to overall combined dataset (marked as *** in above steps)

# Note: will need to replace all "sgs23" with "dat"

# Project ID (static)***
dat %<>% mutate(project_id = "10000007")

# Activity Media Name	(static)***
dat %<>% mutate(activity_media_name = "Water")

# Activity Media Subdivision Name	(static)***
dat %<>% mutate(activity_media_subdivision_name = "Surface Water")

# Activity Start Time Zone***
dat %<>% mutate (activity_start_time_zone = "AKST")

# Activity Latitude	***
# Activity Longitude	***

# read in unqiue lat/long by monitoring location id
lat_long <- read_csv("other/input/2023/misc/lat_long.csv") %>%
  transform(monitoring_location_id = as.character(monitoring_location_id))

# prep overall dataframe for join, then join
dat %<>% 
  mutate(monitoring_location_id = str_trim(monitoring_location_id)) %>%
  left_join(lat_long)

# Activity Depth/Height Measure	***
dat %<>% mutate(activity_depth_height_measure = "15")

# Activity Depth/Height Unit	***
dat %<>% mutate(activity_depth_height_unit = "cm")

# Activity Comment ***
dat %<>% mutate(activity_comment = "")

# Activity Horizontal Collection Method	***
dat %<>% mutate(activity_horizontal_collection_method = "GPS-Unspecified")

# Activity Horizontal Reference Datum	***
dat %<>% mutate(activity_horizontal_reference_datum = "NAD83")

# Result Weight Basis	***
dat %<>% mutate(result_weight_basis = "Sampled")

# Result Value Type	***
dat %<>% mutate(result_value_type = "Actual")

# Sample Collection Equipment Name ***	
dat %<>% mutate(sample_collection_equipment_name = "Water Bottle")

# Thermal Preservative Used	***
dat %<>% mutate(thermal_preservative_used = "Cold Packs (4 deg C)")


```




```{r echo = F}
# Address self-evident critical errors

# A typo from the SGS labs resulted in showing multiple incorrect fieldwork dates for spring 2023. Correct this here
dat %<>%
  
  # correct typo showing field sampling occurring on 2023-05-03 (day after field event)
  mutate(activity_start_date = case_when(
    activity_start_date == "2023-05-03" & sample_type == "PS" ~ "2023-05-02",
    TRUE ~ activity_start_date)) %>%
  
  # here we will correct a typo showing lab QA/QC analyses occurring prior to summer field sampling date on 2023-07-18. By definition no activity could have occurred with these project samples beforehand, as they had not yet been collected
  mutate(activity_start_date = case_when(
    ymd(activity_start_date) < ymd("2023-07-18") &  ymd(activity_start_date) > ymd("2023-07-13")
      & sample_type == "PS" ~ "2023-07-18",
    TRUE ~ activity_start_date)) 


# There are three rows that indicate sample information where no sample was actually collected. These are the fecal coliform and TSS samples from RM 10.1 on 5/2/2023 (site was not visited), and a fecal coliform sample from RM40 on 5/2/2023 that was spilled on site. Remove these extraneous rows.
dat %<>% 
  filter(result_value != "N/A",
         !is.na(result_value))


```




```{r echo = F}

# transform column classes as necessary for QA/QC analyses in this code chunk

  
# prepare time formats for fecal coliform & TSS samples; make consistent with other formats (missing seconds)
dat %<>%
  # activity start time 
  mutate(activity_start_time = case_when(
    characteristic_name == "Fecal Coliform" | # OR operator
      characteristic_name == "Total suspended solids" & 
      sample_type == "PS" ~ paste0(activity_start_time,":00"),
    TRUE ~ activity_start_time)) %>%
  
  # lab receipt time
  mutate(rec_time = case_when(
    characteristic_name == "Fecal Coliform" & sample_type == "PS" ~ paste0(rec_time,":00"),
    TRUE ~ rec_time))


# prepare format for receive time of samples to lab
dat %<>%
  mutate(rec_datetime = ymd_hms(paste(rec_date, rec_time))) %>%
  mutate(rec_date = date(rec_datetime),
         rec_time = as_hms(rec_datetime)) 
  
# prepare format for sample collection date and time
dat %<>%
  mutate(activity_start_datetime = ymd_hms(paste(activity_start_date,activity_start_time))) %>%
  mutate(activity_start_time = as_hms(activity_start_datetime),
         activity_start_date = as.Date(activity_start_datetime))


```


### 2023 Data QA/QC Evaluation

Prior to upload to the EPA WQX, all water quality data must be checked against a series of standard questions in order to evaluate how quality assurance / quality check (QA/QC) requirements are met. The draft Data Evaluation Checklist Template (available for download below) outlines these questions:

```{r, echo = F}
xfun::embed_file('other/misc/qa_qc/Kenai_Baseline_Data_Evaluation_Checklist_2023.xlsx', text = "Download 2023 Draft Kenai Baseline Data Evaluation Checklist Template")

```


Download results pre-QA/QC; including lab QA QC info

```{r echo = F}

# to do:

# 3/4/24 - just noticed: some activity IDs are incorrect because date corrections were made AFTER the activity ID was created

```



#### Pre-database

##### Overall project success

**1.) Were the appropriate analytical methods used for all parameters?**

Yes. Analytical methods from the approved 2023 QAPP were employed, withj one exception (see below).


One change from the approved 2023 occurred. For spring 2023, a laboratory method used to analyze dissolved metals differed slightly from that specified on the QAPP. EPA method 6010D was employed rather than EPA method 200.8. The project manager at SGS Laboratories in Anchorage, AK describes that no differences are anticipated in how results can be interpreted.


<br>

**2.) Were there any deviations from the sampling plan?**

In 2023 there were several small deviations from the sampling plan due to challenging spring field logistics. One site in spring 2023 could not be accessed for sampling due to low water conditions (Kenai River main stem, RM 10.1). As a result there are no samples from this site/date. Additionally, one fecal coliform sample is missing from RM 10.1 on 5/2/2023 due to an on-site loss due to spillage.

<br>

**3.) Were field duplicates, blanks, and/or other field QC samples collected as planned?**

```{r echo = F}
# lets look at this question from a few perspectives

qa_samples1 <- dat %>%
  group_by(activity_start_date,activity_type,characteristic_name,result_analytical_method_id,sample_type, result_sample_fraction) %>%
  count() %>%
    # filter out LABORATORY QA/QC results
  filter(!is.na(activity_start_date),
         !is.na(activity_type)) %>%
  arrange(characteristic_name, activity_type) %>%
  filter(sample_type == "PS")

# another way
qa_samples2 <- dat %>%
  group_by(activity_start_date,result_analytical_method_id,sample_type) %>%

  count() %>%
  filter(sample_type == "PS")


# more exacting discussion here TBD, but in general pretty close

```


<br>


**4.)	Do the laboratory reports provide results for all sites and parameters?**

The laboratory reports from both labs provide results corresponding with the chain of custody documents provided, within the parameters of what is described in questions #1 - #3.

<br>

**5.)	Is a copy of the COC included with the laboratory reports?**

Yes. Chain of custody documents from SGS laboratories are included in the results PDFs linked above. Chain of custody documents from the Soldotna Wastewater Treatment Plant are linked below:

```{r echo = F}

embed_dir("other/misc/qa_qc/chain_of_custody_2023", text = "Download 2023 Chain of Custody Documents from Soldotna Wastewater Treatment Plant")

```


<br>

**6.)	Do the laboratory reports match the COC and requested methods throughout?	**

Yes, with one exception discussed in question #1. For spring 2023, SGS laboratories used EPA method 6010D for dissolved metals rather than EPA method 200.8. The SGS project manager describes that no change is anticipated for how to interpret results.

<br>

**7.)	Are the number of samples on the laboratory reports the same as on the COC?	**

Yes, no loss of samples occurred in the process of generating laboratory results.

<br>

**8.)	Was all supporting info provided in the laboratory report, such as reporting limits for all analyses and definitions?	**

SGS laboratory reports include reporting limits in both the electronic data deliverables as well as the PDF reports. Results from the Soldotna Wastewater Treatment Plant do not contain reporting limits. Reporting limits and other QA/QC data considerations are provided in the 2023 QAPP.


<br>

**9.)	Are site names, dates, and times correct and as expected?	**

Numerous corrections to site names and dates were necessary to prepare data into final form. Original documents were not modified, and corrections are applied throughout the data read-in process using R coding script.

<br>

**10.)	Were there any issues with instrument calibration?	**

No issues are documented with calibration for KWF's field or labortory instruments. No calibration issues are described in the "Case Narrative" sections of the spring and summer SGS PDF laboratory reports.


<br>

**11.)	Did the instrument perform as expected?	**

TBD with KWF results.

<br>

**12.)	Was instrument calibration performed according to the QAPP and instrument recommendations?	**

Yes.

<br>


**13.)	Was instrument verification during the field season performed according to the QAPP and instrument recommendations? 	**

For water temperature probes, instrument verification rather than calibration is performed. An ice-bath test was performed on all water temperature probes according to methods described in Mauger et al. 2015 prior to field use, and all probes recorded values within 0.25 degress celsius of a NIST certified thermometer.

<br>

**14.)	Were instrument calibration verification logs or records kept?	**

Calibration and verification records are kepat at Kenai Watershed Forum's office at 44129 Sterling Hwy, Soldotna, AK and are available upon request.

<br>

**15.)	Do the instrument data files site IDs, time stamps and file names match? 	**

Instrument data files from fieldwork are not generated as part of this project.

<br>

**16.)	Is any in-situ field data rejected and why? 	**

TBD

<br>

**17.)	Were preservation, hold time and temperature requirements met? 	**


```{r echo = F}

# calculate hold times
hold_times <- dat %>%
  mutate(hold_time = as_hms(rec_datetime - activity_start_datetime)) %>%
  mutate(hold_time_minutes = as.numeric(seconds(hold_time))/60) %>%
  mutate(hold_time_hours = hold_time_minutes/60) %>%
  filter(sample_type == "PS") %>%
  select(-hold_time,-hold_time_minutes)

# read in max hold times
sample_holding_times <- read.csv("other/misc/qa_qc/sample_holding_times.csv") %>%
  filter(!is.na(max_holding_time_hours)) %>%
  transform(max_holding_time_hours = as.numeric(max_holding_time_hours)) %>%
  select(-max_holding_time_text)

hold_times <- left_join(hold_times,sample_holding_times) %>%
  mutate(hold_time_pass = case_when(
    max_holding_time_hours < hold_time_hours ~ "N",
    TRUE ~ "Y"))

# one sample was warm... (MB and MSD; internal lab notes)

# working here 3/5/24

```

We calculated time from sample field collection to lab receipt as documented in 2023 lab reports.


<br>

**	18.)	Are dissolved metal quantities less than total metals quantities?	**

<br>

**	19.)	Are  the duplicate sample(s) RPD within range described in QAPP?	**

<br>

**	20.)	Were there any laboratory discrepancies, errors, data qualifiers, or QC failures (review laboratory duplicates, matrix spikes and blanks)? 	**

<br>

**	21.)	Is any laboratory data rejected and why? 	**

<br>


**	22.)	During the field season, review raw data files (EDDs, instrument records) as they are received. Document changes and corrections to methods as needed.	**


<br>

**	23.)	Is the dataset  complete and did you receive the expected number of results?	**

<br>

**	24.)	Was the data collected representative of environmental conditions?	**

<br>

**	25.)	Does project meet Completeness Measure A criteria?	**

<br>

**	26.)	Does project meet Completeness Measure B criteria?	**

<br>


**	27.)	Was the QA Officer consulted for any data concerns? 	**

<br>


#### Database Prep


**	28.)	Are the correct monitoring locations associated with the project?	**

<br>


**	29.)	Are the QAPP and other supporting documents attached?	**

<br>


**	30.)	Is all metadata correct?	**

<br>

**	31.)	Is the organization ID correct?	**

<br>

**	32.)	 Are the time zones consistent and correct (AKDT in summer)?	**

<br>

**	33.)	 Are all media types included? Media types appropriate to Characteristic?	**

<br>

**	34.)	Check Sample Collection, Preparation and Preservation Methods, Thermal Preservative, Equipment ID, Activity Media. Is supporting information included and correct?	**

<br>

**	35.)	Are all expected activity types present and are QC samples correctly identified?	**

<br>

**	36.)	Is the Activity media subdivision filled in (if relevant)?	**

<br>

**	37.)	For Water activity media, is the relative depth filled in? 	**

<br>

**	38.)	Is the number of results for each Characteristic correct? 	**

<br>

**	39.)	Do the range of result values make sense?	**

<br>

**	40.)	Are units correct and consistent for each parameter?	**

<br>

**	41.)	Are detection limits and laboratory qualifiers included for analytical results? 	**











```{r}
### Part E: QA/QC

# questions to add to existing list:

# # Are sample collection dates unique to the actual recorded dates?
# (no; spring has two dates; lab typo)


# are fecal coliform "0" values listed as "NA"? (they should be)


```










